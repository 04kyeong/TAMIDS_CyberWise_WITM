{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Eh26LG0Aba7",
        "outputId": "cc32a60a-d5de-4d23-cedc-684b5d9303fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting IoT Cyber Attack Classification Model Training\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Output directories '/content/drive/My Drive/Data_2023/Model_Figures_1' and '/content/drive/My Drive/Data_2023/Models_1' are ready.\n",
            "Found 1 CSV files in the directory.\n",
            "Loading Merged01.csv...\n",
            "Combined dataset shape: (712311, 40)\n",
            "\n",
            "--- Preprocessing Data ---\n",
            "Initial data shape: (712311, 40)\n",
            "Detected label column: Label\n",
            "Total missing values: 22\n",
            "Missing values handled. Remaining missing: 0\n",
            "Processed data shape: (712311, 42)\n",
            "Attack types: ['DDOS-PSHACK_FLOOD', 'MIRAI-GREIP_FLOOD', 'DOS-UDP_FLOOD', 'DNS_SPOOFING', 'DDOS-ICMP_FLOOD', 'DDOS-TCP_FLOOD', 'DDOS-SYN_FLOOD', 'DDOS-UDP_FLOOD', 'MITM-ARPSPOOFING', 'DDOS-SYNONYMOUSIP_FLOOD', 'DOS-TCP_FLOOD', 'VULNERABILITYSCAN', 'DOS-SYN_FLOOD', 'DDOS-RSTFINFLOOD', 'BENIGN', 'DDOS-SLOWLORIS', 'DDOS-ICMP_FRAGMENTATION', 'MIRAI-GREETH_FLOOD', 'RECON-HOSTDISCOVERY', 'MIRAI-UDPPLAIN', 'RECON-PORTSCAN', 'DDOS-ACK_FRAGMENTATION', 'DDOS-UDP_FRAGMENTATION', 'RECON-OSSCAN', 'BACKDOOR_MALWARE', 'DOS-HTTP_FLOOD', 'XSS', 'DDOS-HTTP_FLOOD', 'BROWSERHIJACKING', 'SQLINJECTION', 'DICTIONARYBRUTEFORCE', 'COMMANDINJECTION', 'RECON-PINGSWEEP', 'UPLOADING_ATTACK']\n",
            "Label mapping: {np.int64(0): 'BACKDOOR_MALWARE', np.int64(1): 'BENIGN', np.int64(2): 'BROWSERHIJACKING', np.int64(3): 'COMMANDINJECTION', np.int64(4): 'DDOS-ACK_FRAGMENTATION', np.int64(5): 'DDOS-HTTP_FLOOD', np.int64(6): 'DDOS-ICMP_FLOOD', np.int64(7): 'DDOS-ICMP_FRAGMENTATION', np.int64(8): 'DDOS-PSHACK_FLOOD', np.int64(9): 'DDOS-RSTFINFLOOD', np.int64(10): 'DDOS-SLOWLORIS', np.int64(11): 'DDOS-SYNONYMOUSIP_FLOOD', np.int64(12): 'DDOS-SYN_FLOOD', np.int64(13): 'DDOS-TCP_FLOOD', np.int64(14): 'DDOS-UDP_FLOOD', np.int64(15): 'DDOS-UDP_FRAGMENTATION', np.int64(16): 'DICTIONARYBRUTEFORCE', np.int64(17): 'DNS_SPOOFING', np.int64(18): 'DOS-HTTP_FLOOD', np.int64(19): 'DOS-SYN_FLOOD', np.int64(20): 'DOS-TCP_FLOOD', np.int64(21): 'DOS-UDP_FLOOD', np.int64(22): 'MIRAI-GREETH_FLOOD', np.int64(23): 'MIRAI-GREIP_FLOOD', np.int64(24): 'MIRAI-UDPPLAIN', np.int64(25): 'MITM-ARPSPOOFING', np.int64(26): 'RECON-HOSTDISCOVERY', np.int64(27): 'RECON-OSSCAN', np.int64(28): 'RECON-PINGSWEEP', np.int64(29): 'RECON-PORTSCAN', np.int64(30): 'SQLINJECTION', np.int64(31): 'UPLOADING_ATTACK', np.int64(32): 'VULNERABILITYSCAN', np.int64(33): 'XSS'}\n",
            "Dataset is very large (712311 rows). Sampling 300,000 rows for model training.\n",
            "\n",
            "--- Training and Evaluating Models ---\n",
            "Training set shape: (240000, 39)\n",
            "Test set shape: (60000, 39)\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest - Accuracy: 0.7732, F1 Score: 0.7462, Training Time: 13.13s\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost - Accuracy: 0.7842, F1 Score: 0.7711, Training Time: 60.81s\n",
            "\n",
            "Training Neural Network...\n",
            "Neural Network - Accuracy: 0.7767, F1 Score: 0.7556, Training Time: 470.92s\n",
            "\n",
            "--- Comparing Model Performance ---\n",
            "Model comparison visualization saved.\n",
            "\n",
            "Best model based on F1 score: XGBoost\n",
            "\n",
            "--- Analyzing Feature Importance ---\n",
            "Feature importance analysis completed for XGBoost\n",
            "\n",
            "--- Creating Model Summary ---\n",
            "Model summary created.\n",
            "\n",
            "--- Model Training and Evaluation Complete ---\n",
            "Best model: XGBoost\n",
            "All models and visualizations have been saved to the '/content/drive/My Drive/Data_2023/Models_1' and '/content/drive/My Drive/Data_2023/Model_Figures_1' directories.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "IoT Cyber Attack Classification Model for Google Colab (RF, XGBoost, NN only)\n",
        "------------------------------------------------------\n",
        "This script trains and evaluates selected machine learning models (Random Forest,\n",
        "XGBoost, and Neural Network) for classifying IoT cyber attacks, and provides\n",
        "visualizations of model performance.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up styling for plots\n",
        "plt.style.use('ggplot')\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "def mount_drive():\n",
        "    \"\"\"Mount Google Drive and return the base path.\"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "    return '/content/drive'\n",
        "\n",
        "def create_output_dirs(base_path):\n",
        "    \"\"\"Create output directories for figures and models if they don't exist.\"\"\"\n",
        "    figures_dir = os.path.join(base_path, 'My Drive/Data_2023/Model_Figures_1')\n",
        "    models_dir = os.path.join(base_path, 'My Drive/Data_2023/Models_1')\n",
        "\n",
        "    for dir_path in [figures_dir, models_dir]:\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "\n",
        "    print(f\"Output directories '{figures_dir}' and '{models_dir}' are ready.\")\n",
        "    return figures_dir, models_dir\n",
        "\n",
        "def load_data(base_path):\n",
        "    \"\"\"Load all CSV files from the specified directory and concatenate them.\"\"\"\n",
        "    # Path to data folder\n",
        "    data_folder = os.path.join(base_path, 'My Drive/Data_2023')\n",
        "\n",
        "    # Get all CSV files\n",
        "    all_files = glob.glob(os.path.join(data_folder, '*1.csv'))\n",
        "\n",
        "    if not all_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {data_folder}\")\n",
        "\n",
        "    print(f\"Found {len(all_files)} CSV files in the directory.\")\n",
        "\n",
        "    # Initialize an empty list to store each dataframe\n",
        "    dfs = []\n",
        "\n",
        "    # Loop through each CSV file and load it into a dataframe\n",
        "    for file in all_files:\n",
        "        try:\n",
        "            print(f\"Loading {os.path.basename(file)}...\")\n",
        "            df = pd.read_csv(file)\n",
        "            dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    # Concatenate all dataframes\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No valid CSV files could be loaded.\")\n",
        "\n",
        "    data = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"Combined dataset shape: {data.shape}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def preprocess_data(df, models_dir):\n",
        "    \"\"\"Clean and preprocess the data for modeling.\"\"\"\n",
        "    print(\"\\n--- Preprocessing Data ---\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    data = df.copy()\n",
        "\n",
        "    # Display initial info\n",
        "    print(f\"Initial data shape: {data.shape}\")\n",
        "\n",
        "    # Get the label column (assuming it's 'Label' or the last column)\n",
        "    if 'Label' in data.columns:\n",
        "        label_column = 'Label'\n",
        "    else:\n",
        "        label_column = data.columns[-1]\n",
        "\n",
        "    print(f\"Detected label column: {label_column}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = data.isnull().sum().sum()\n",
        "    print(f\"Total missing values: {missing_values}\")\n",
        "\n",
        "    if missing_values > 0:\n",
        "        # Fill numeric columns with their median\n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "        # Fill categorical columns with mode\n",
        "        cat_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "        for col in cat_cols:\n",
        "            data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "        print(f\"Missing values handled. Remaining missing: {data.isnull().sum().sum()}\")\n",
        "\n",
        "    # Handle potential infinite values\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "    # Identify and convert categorical columns to numeric\n",
        "    cat_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Remove label column from this list if it's categorical\n",
        "    if label_column in cat_cols:\n",
        "        cat_cols.remove(label_column)\n",
        "\n",
        "    # OneHot encoding for categorical features with low cardinality\n",
        "    for col in cat_cols:\n",
        "        if data[col].nunique() < 10:  # Only one-hot encode if fewer than 10 unique values\n",
        "            one_hot = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
        "            data = pd.concat([data, one_hot], axis=1)\n",
        "            data.drop(col, axis=1, inplace=True)\n",
        "        else:\n",
        "            # For high cardinality, use label encoding\n",
        "            le = LabelEncoder()\n",
        "            data[col] = le.fit_transform(data[col].astype(str))\n",
        "\n",
        "    # Encode the label column\n",
        "    le = LabelEncoder()\n",
        "    data['attack_encoded'] = le.fit_transform(data[label_column])\n",
        "\n",
        "    # Create a mapping of encoded values to original labels\n",
        "    label_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
        "\n",
        "    # Keep the original label for reference\n",
        "    data['attack_label'] = data[label_column]\n",
        "\n",
        "    # Save label encoder for future predictions\n",
        "    joblib.dump(le, os.path.join(models_dir, 'label_encoder.joblib'))\n",
        "\n",
        "    print(f\"Processed data shape: {data.shape}\")\n",
        "    print(f\"Attack types: {data['attack_label'].unique().tolist()}\")\n",
        "    print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "    return data, label_mapping\n",
        "\n",
        "def train_and_evaluate_models(df, label_mapping, figures_dir, models_dir):\n",
        "    \"\"\"Train and evaluate selected models for attack classification.\"\"\"\n",
        "    print(\"\\n--- Training and Evaluating Models ---\")\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = df.select_dtypes(include=[np.number]).drop(['attack_encoded'], axis=1)\n",
        "    y = df['attack_encoded']\n",
        "\n",
        "    # Get feature names\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # Save feature names for later use\n",
        "    with open(os.path.join(models_dir, 'feature_names.txt'), 'w') as f:\n",
        "        for feature in feature_names:\n",
        "            f.write(f\"{feature}\\n\")\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, os.path.join(models_dir, 'scaler.joblib'))\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "    # Define only the selected models to evaluate (RF, XGBoost, NN)\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
        "        'XGBoost': XGBClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1),\n",
        "        'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "    }\n",
        "\n",
        "    # Results storage\n",
        "    results = {}\n",
        "    training_times = {}\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Record training time\n",
        "            training_time = time.time() - start_time\n",
        "            training_times[name] = training_time\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Evaluate the model\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            recall = recall_score(y_test, y_pred, average='weighted')\n",
        "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "            results[name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1,\n",
        "                'training_time': training_time\n",
        "            }\n",
        "\n",
        "            print(f\"{name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Training Time: {training_time:.2f}s\")\n",
        "\n",
        "            # Save the model\n",
        "            joblib.dump(model, os.path.join(models_dir, f\"{name.replace(' ', '_').lower()}.joblib\"))\n",
        "\n",
        "            # Plot confusion matrix for top attack types\n",
        "            # Limit to top 10 attack types for readability\n",
        "            attack_counts = df['attack_encoded'].value_counts()\n",
        "            top_attacks = attack_counts.nlargest(10).index.tolist()\n",
        "\n",
        "            # Filter test data to only include top attacks\n",
        "            top_attacks_mask = np.isin(y_test, top_attacks)\n",
        "            y_test_top = y_test[top_attacks_mask]\n",
        "            y_pred_top = y_pred[top_attacks_mask]\n",
        "\n",
        "            if len(y_test_top) > 0:\n",
        "                cm = confusion_matrix(y_test_top, y_pred_top)\n",
        "                plt.figure(figsize=(14, 12))\n",
        "\n",
        "                # Get class names for the top attacks\n",
        "                class_names = [label_mapping[i] for i in top_attacks if i in y_test_top.unique()]\n",
        "\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                           xticklabels=class_names,\n",
        "                           yticklabels=class_names)\n",
        "                plt.title(f'Confusion Matrix - {name} (Top Attack Types)')\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('Actual')\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "                plt.yticks(rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(figures_dir, f'confusion_matrix_{name.replace(\" \", \"_\").lower()}.png'))\n",
        "                plt.close()\n",
        "\n",
        "            # Generate classification report\n",
        "            report = classification_report(y_test, y_pred, target_names=[label_mapping[i] for i in sorted(label_mapping.keys())], output_dict=True)\n",
        "\n",
        "            # Save the report as a dataframe\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            report_df.to_csv(os.path.join(models_dir, f'classification_report_{name.replace(\" \", \"_\").lower()}.csv'))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Compare models performance\n",
        "    if results:\n",
        "        compare_models(results, training_times, figures_dir, models_dir)\n",
        "\n",
        "        # Select the best model based on F1 score\n",
        "        best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
        "        print(f\"\\nBest model based on F1 score: {best_model_name}\")\n",
        "\n",
        "        # Analyze feature importance for the best model if it's tree-based\n",
        "        if best_model_name in ['Random Forest', 'XGBoost']:\n",
        "            try:\n",
        "                # Load the best model\n",
        "                best_model = joblib.load(os.path.join(models_dir, f\"{best_model_name.replace(' ', '_').lower()}.joblib\"))\n",
        "                analyze_feature_importance(best_model, feature_names, best_model_name, figures_dir)\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing feature importance: {e}\")\n",
        "\n",
        "        return best_model_name\n",
        "    else:\n",
        "        print(\"No models were successfully trained.\")\n",
        "        return None\n",
        "\n",
        "def compare_models(results, training_times, figures_dir, models_dir):\n",
        "    \"\"\"Compare model performance metrics.\"\"\"\n",
        "    print(\"\\n--- Comparing Model Performance ---\")\n",
        "\n",
        "    # Create dataframe from results\n",
        "    results_df = pd.DataFrame(results).transpose()\n",
        "\n",
        "    # Plot model comparison\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
        "\n",
        "    # Performance metrics plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    results_df[metrics].plot(kind='bar', ax=ax)\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0.7, 1.0)  # Adjust as needed for better visualization\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(figures_dir, 'model_performance_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Training time plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(training_times.keys(), training_times.values(), color='skyblue')\n",
        "    plt.title('Model Training Time Comparison')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Training Time (seconds)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(figures_dir, 'model_training_time.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df.to_csv(os.path.join(models_dir, 'model_performance_comparison.csv'))\n",
        "\n",
        "    print(\"Model comparison visualization saved.\")\n",
        "\n",
        "def analyze_feature_importance(model, feature_names, model_name, figures_dir):\n",
        "    \"\"\"Analyze and visualize feature importance for the best model.\"\"\"\n",
        "    print(\"\\n--- Analyzing Feature Importance ---\")\n",
        "\n",
        "    # Get feature importances (works for tree-based models)\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        try:\n",
        "            # Create dataframe of feature importances\n",
        "            feature_importances = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            # Visualize top 20 features\n",
        "            top_n = min(20, len(feature_importances))\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            top_features = feature_importances.head(top_n)\n",
        "            sns.barplot(x='importance', y='feature', data=top_features)\n",
        "            plt.title(f'Top {top_n} Features by Importance ({model_name})')\n",
        "            plt.xlabel('Importance')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(figures_dir, 'top_features_importance.png'))\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"Feature importance analysis completed for {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in feature importance visualization: {e}\")\n",
        "    else:\n",
        "        print(f\"Feature importance analysis not applicable for {model_name}\")\n",
        "\n",
        "def create_model_summary(models_dir):\n",
        "    \"\"\"Create a summary of model performance and recommendations.\"\"\"\n",
        "    print(\"\\n--- Creating Model Summary ---\")\n",
        "\n",
        "    # Load model performance data\n",
        "    try:\n",
        "        model_performance_path = os.path.join(models_dir, 'model_performance_comparison.csv')\n",
        "        if os.path.exists(model_performance_path):\n",
        "            model_performance = pd.read_csv(model_performance_path)\n",
        "            model_performance.set_index('Unnamed: 0', inplace=True)\n",
        "            model_performance.index.name = 'Model'\n",
        "\n",
        "            # Identify best model based on F1 score\n",
        "            best_model = model_performance['f1_score'].idxmax()\n",
        "            best_f1 = model_performance.loc[best_model, 'f1_score']\n",
        "            best_accuracy = model_performance.loc[best_model, 'accuracy']\n",
        "\n",
        "            # Create summary text file\n",
        "            with open(os.path.join(models_dir, 'model_summary.txt'), 'w') as f:\n",
        "                f.write(\"IoT Cyber Attack Classification Model Summary\\n\")\n",
        "                f.write(\"=============================================\\n\\n\")\n",
        "\n",
        "                f.write(\"Model Performance Comparison:\\n\")\n",
        "                f.write(\"--------------------------\\n\")\n",
        "                f.write(model_performance.to_string())\n",
        "                f.write(\"\\n\\n\")\n",
        "\n",
        "                f.write(\"Best Performing Model:\\n\")\n",
        "                f.write(\"-------------------\\n\")\n",
        "                f.write(f\"Model: {best_model}\\n\")\n",
        "                f.write(f\"F1 Score: {best_f1:.4f}\\n\")\n",
        "                f.write(f\"Accuracy: {best_accuracy:.4f}\\n\\n\")\n",
        "\n",
        "                f.write(\"Recommendation:\\n\")\n",
        "                f.write(\"--------------\\n\")\n",
        "                f.write(f\"Based on the evaluation metrics, the {best_model} model is recommended for IoT cyber attack classification.\\n\")\n",
        "                f.write(\"This model provides the best balance between precision and recall (F1 score).\\n\\n\")\n",
        "\n",
        "                f.write(\"Model Usage Instructions:\\n\")\n",
        "                f.write(\"-----------------------\\n\")\n",
        "                f.write(\"To use this model for prediction:\\n\")\n",
        "                f.write(\"1. Load the model: model = joblib.load('./models/[model_filename].joblib')\\n\")\n",
        "                f.write(\"2. Load the scaler: scaler = joblib.load('./models/scaler.joblib')\\n\")\n",
        "                f.write(\"3. Preprocess new data (same features as training data)\\n\")\n",
        "                f.write(\"4. Scale the features: X_scaled = scaler.transform(X_new)\\n\")\n",
        "                f.write(\"5. Make predictions: predictions = model.predict(X_scaled)\\n\")\n",
        "                f.write(\"6. Convert numeric predictions to labels using the label encoder: joblib.load('./models/label_encoder.joblib')\\n\")\n",
        "\n",
        "            print(\"Model summary created.\")\n",
        "        else:\n",
        "            print(\"Model performance comparison file not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating model summary: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute the model training and evaluation pipeline.\"\"\"\n",
        "    print(\"Starting IoT Cyber Attack Classification Model Training\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    base_path = mount_drive()\n",
        "\n",
        "    # Create output directories\n",
        "    figures_dir, models_dir = create_output_dirs(base_path)\n",
        "\n",
        "    try:\n",
        "        # Load data from all CSV files\n",
        "        data = load_data(base_path)\n",
        "\n",
        "        # Preprocess the data\n",
        "        processed_data, label_mapping = preprocess_data(data, models_dir)\n",
        "\n",
        "        # Sample the data if it's too large for Google Colab memory\n",
        "        if len(processed_data) > 300000:  # Adjust threshold based on your Colab RAM\n",
        "            print(f\"Dataset is very large ({len(processed_data)} rows). Sampling 300,000 rows for model training.\")\n",
        "            processed_data = processed_data.sample(300000, random_state=42)\n",
        "\n",
        "        # Train and evaluate models\n",
        "        best_model = train_and_evaluate_models(processed_data, label_mapping, figures_dir, models_dir)\n",
        "\n",
        "        # Create model summary\n",
        "        create_model_summary(models_dir)\n",
        "\n",
        "        print(\"\\n--- Model Training and Evaluation Complete ---\")\n",
        "        if best_model:\n",
        "            print(f\"Best model: {best_model}\")\n",
        "        print(f\"All models and visualizations have been saved to the '{models_dir}' and '{figures_dir}' directories.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training and evaluation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}