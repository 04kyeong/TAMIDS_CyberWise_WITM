{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "BnvJxCBLufDX",
        "outputId": "e181041a-7616-4bb8-e540-4115227e1ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting IoT Cyber Attack Analysis\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "Output directory '/content/drive/My Drive/Data_2023/figures' is ready.\n",
            "Found 1 CSV files in the directory.\n",
            "Loading Merged01.csv...\n",
            "Combined dataset shape: (712311, 41)\n",
            "\n",
            "--- Preprocessing Data ---\n",
            "Initial data shape: (712311, 41)\n",
            "Columns: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Variance', 'Label', 'source_file']\n",
            "Detected label column: Label\n",
            "Total missing values: 22\n",
            "Missing values handled. Remaining missing: 0\n",
            "Processed data shape: (712311, 42)\n",
            "Attack types: ['DDOS-PSHACK_FLOOD', 'MIRAI-GREIP_FLOOD', 'DOS-UDP_FLOOD', 'DNS_SPOOFING', 'DDOS-ICMP_FLOOD', 'DDOS-TCP_FLOOD', 'DDOS-SYN_FLOOD', 'DDOS-UDP_FLOOD', 'MITM-ARPSPOOFING', 'DDOS-SYNONYMOUSIP_FLOOD', 'DOS-TCP_FLOOD', 'VULNERABILITYSCAN', 'DOS-SYN_FLOOD', 'DDOS-RSTFINFLOOD', 'BENIGN', 'DDOS-SLOWLORIS', 'DDOS-ICMP_FRAGMENTATION', 'MIRAI-GREETH_FLOOD', 'RECON-HOSTDISCOVERY', 'MIRAI-UDPPLAIN', 'RECON-PORTSCAN', 'DDOS-ACK_FRAGMENTATION', 'DDOS-UDP_FRAGMENTATION', 'RECON-OSSCAN', 'BACKDOOR_MALWARE', 'DOS-HTTP_FLOOD', 'XSS', 'DDOS-HTTP_FLOOD', 'BROWSERHIJACKING', 'SQLINJECTION', 'DICTIONARYBRUTEFORCE', 'COMMANDINJECTION', 'RECON-PINGSWEEP', 'UPLOADING_ATTACK']\n",
            "\n",
            "--- Performing Exploratory Data Analysis ---\n",
            "PCA completed. Total explained variance: 25.71%\n",
            "\n",
            "--- Performing Correlation Analysis ---\n",
            "Found 54 feature pairs with high correlation (|r| > 0.5)\n",
            "Feature-attack correlation analysis completed for 34 attack types\n",
            "\n",
            "--- Analyzing Feature Importance (Optimized) ---\n",
            "Dataset is large (712311 rows). Using a sample of 100000 rows for feature importance analysis.\n",
            "Using a subset of 10,000 samples for evaluation\n",
            "Random Forest model accuracy on sample: 0.7720\n",
            "\n",
            "--- Creating Summary Dashboard ---\n",
            "Summary dashboard created\n",
            "\n",
            "--- Analysis Complete ---\n",
            "All visualizations have been saved to the '/content/drive/My Drive/Data_2023/figures' directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "IoT Cyber Attack Analysis for Google Colab\n",
        "-----------------------------------------\n",
        "This script analyzes datasets of cyber attacks on IoT systems, performs\n",
        "feature correlation analysis, and generates visualizations to help identify\n",
        "patterns in different attack types.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.colors import ListedColormap\n",
        "from collections import Counter\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up styling for plots\n",
        "plt.style.use('ggplot')\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "SMALL_SIZE = 10\n",
        "MEDIUM_SIZE = 12\n",
        "BIGGER_SIZE = 14\n",
        "plt.rc('font', size=SMALL_SIZE)\n",
        "plt.rc('axes', titlesize=BIGGER_SIZE)\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
        "\n",
        "def mount_drive():\n",
        "    \"\"\"Mount Google Drive and return the base path.\"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "    return '/content/drive'\n",
        "\n",
        "def create_output_dir(base_path):\n",
        "    \"\"\"Create output directory for figures if it doesn't exist.\"\"\"\n",
        "    figures_dir = os.path.join(base_path, 'My Drive/Data_2023/figures')\n",
        "    if not os.path.exists(figures_dir):\n",
        "        os.makedirs(figures_dir)\n",
        "    print(f\"Output directory '{figures_dir}' is ready.\")\n",
        "    return figures_dir\n",
        "\n",
        "def load_data(base_path):\n",
        "    \"\"\"Load all CSV files from the specified directory and concatenate them.\"\"\"\n",
        "    # Path to data folder\n",
        "    data_folder = os.path.join(base_path, 'My Drive/Data_2023')\n",
        "\n",
        "    # Get all CSV files\n",
        "    all_files = glob.glob(os.path.join(data_folder, '*1.csv'))\n",
        "\n",
        "    if not all_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {data_folder}\")\n",
        "\n",
        "    print(f\"Found {len(all_files)} CSV files in the directory.\")\n",
        "\n",
        "    # Initialize an empty list to store each dataframe\n",
        "    dfs = []\n",
        "\n",
        "    # Loop through each CSV file and load it into a dataframe\n",
        "    for file in all_files:\n",
        "        try:\n",
        "            print(f\"Loading {os.path.basename(file)}...\")\n",
        "            df = pd.read_csv(file)\n",
        "            # Add file source as a column for reference\n",
        "            df['source_file'] = os.path.basename(file)\n",
        "            dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    # Concatenate all dataframes\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No valid CSV files could be loaded.\")\n",
        "\n",
        "    data = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"Combined dataset shape: {data.shape}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Clean and preprocess the data.\"\"\"\n",
        "    print(\"\\n--- Preprocessing Data ---\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    data = df.copy()\n",
        "\n",
        "    # Display initial info\n",
        "    print(f\"Initial data shape: {data.shape}\")\n",
        "    print(f\"Columns: {data.columns.tolist()}\")\n",
        "\n",
        "    # Get the label column (assuming it's 'Label' or the last column before 'source_file')\n",
        "    if 'Label' in data.columns:\n",
        "        label_column = 'Label'\n",
        "    else:\n",
        "        # If 'Label' is not present, use the last column before 'source_file'\n",
        "        if 'source_file' in data.columns:\n",
        "            label_column = data.columns[-2]\n",
        "        else:\n",
        "            label_column = data.columns[-1]\n",
        "\n",
        "    print(f\"Detected label column: {label_column}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = data.isnull().sum().sum()\n",
        "    print(f\"Total missing values: {missing_values}\")\n",
        "\n",
        "    if missing_values > 0:\n",
        "        # Fill numeric columns with their median\n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "        # Fill categorical columns with mode\n",
        "        cat_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "        for col in cat_cols:\n",
        "            data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "        print(f\"Missing values handled. Remaining missing: {data.isnull().sum().sum()}\")\n",
        "\n",
        "    # Handle potential infinite values\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "    # Identify and convert categorical columns to numeric\n",
        "    cat_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Remove 'source_file' and label column from this list\n",
        "    if 'source_file' in cat_cols:\n",
        "        cat_cols.remove('source_file')\n",
        "    if label_column in cat_cols:\n",
        "        cat_cols.remove(label_column)\n",
        "\n",
        "    # OneHot encoding for categorical features with low cardinality\n",
        "    for col in cat_cols:\n",
        "        if data[col].nunique() < 10:  # Only one-hot encode if fewer than 10 unique values\n",
        "            one_hot = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
        "            data = pd.concat([data, one_hot], axis=1)\n",
        "            data.drop(col, axis=1, inplace=True)\n",
        "        else:\n",
        "            # For high cardinality, use label encoding\n",
        "            le = LabelEncoder()\n",
        "            data[col] = le.fit_transform(data[col].astype(str))\n",
        "\n",
        "    # Encode the label column\n",
        "    le = LabelEncoder()\n",
        "    data['attack_encoded'] = le.fit_transform(data[label_column])\n",
        "    # Create a mapping of encoded values to original labels\n",
        "    label_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
        "\n",
        "    # Keep the label column for reference\n",
        "    data['attack_label'] = data[label_column]\n",
        "\n",
        "    # Remove any unnecessary columns\n",
        "    if 'source_file' in data.columns:\n",
        "        data.drop('source_file', axis=1, inplace=True)\n",
        "\n",
        "    print(f\"Processed data shape: {data.shape}\")\n",
        "    print(f\"Attack types: {data['attack_label'].unique().tolist()}\")\n",
        "\n",
        "    return data, label_mapping\n",
        "\n",
        "def exploratory_data_analysis(df, label_mapping, figures_dir):\n",
        "    \"\"\"Perform exploratory data analysis and generate visualizations.\"\"\"\n",
        "    print(\"\\n--- Performing Exploratory Data Analysis ---\")\n",
        "\n",
        "    # Count of each attack type\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    attack_counts = df['attack_label'].value_counts()\n",
        "\n",
        "    # Create a more readable plot with percentages\n",
        "    ax = sns.barplot(x=attack_counts.index, y=attack_counts.values)\n",
        "    plt.title('Distribution of Attack Types')\n",
        "    plt.xlabel('Attack Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add percentage labels\n",
        "    total = len(df)\n",
        "    for i, p in enumerate(ax.patches):\n",
        "        percentage = f'{100 * p.get_height() / total:.1f}%'\n",
        "        ax.annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                     ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(figures_dir, 'attack_type_distribution.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Distribution of protocol types by attack\n",
        "    if 'Protocol' in df.columns:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        protocol_by_attack = pd.crosstab(df['attack_label'], df['Protocol'])\n",
        "        protocol_by_attack_pct = protocol_by_attack.div(protocol_by_attack.sum(axis=1), axis=0) * 100\n",
        "\n",
        "        protocol_by_attack_pct.plot(kind='bar', stacked=True, colormap='viridis')\n",
        "        plt.title('Protocol Distribution by Attack Type')\n",
        "        plt.xlabel('Attack Type')\n",
        "        plt.ylabel('Percentage (%)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.legend(title='Protocol', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(figures_dir, 'protocol_by_attack.png'))\n",
        "        plt.close()\n",
        "\n",
        "    # Flow Duration by attack type (boxplot)\n",
        "    if 'Flow Duration' in df.columns:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "\n",
        "        # Use log scale for better visualization\n",
        "        sns.boxplot(x='attack_label', y='Flow Duration', data=df)\n",
        "        plt.yscale('log')\n",
        "        plt.title('Flow Duration by Attack Type')\n",
        "        plt.xlabel('Attack Type')\n",
        "        plt.ylabel('Flow Duration (log scale)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(figures_dir, 'flow_duration_by_attack.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Flow Duration distribution by attack type (violin plot)\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        sns.violinplot(x='attack_label', y='Flow Duration', data=df, inner='quart', cut=0)\n",
        "        plt.yscale('log')\n",
        "        plt.title('Flow Duration Distribution by Attack Type')\n",
        "        plt.xlabel('Attack Type')\n",
        "        plt.ylabel('Flow Duration (log scale)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(figures_dir, 'flow_duration_violin.png'))\n",
        "        plt.close()\n",
        "\n",
        "    # Analyze packet counts if those columns exist\n",
        "    packet_cols = [col for col in df.columns if 'Packet' in col]\n",
        "    if packet_cols:\n",
        "        # Packet counts by attack type\n",
        "        fig, axes = plt.subplots(nrows=len(packet_cols), figsize=(14, 5*len(packet_cols)))\n",
        "\n",
        "        if len(packet_cols) == 1:\n",
        "            axes = [axes]  # Make it iterable if only one subplot\n",
        "\n",
        "        for i, col in enumerate(packet_cols):\n",
        "            sns.boxplot(x='attack_label', y=col, data=df, ax=axes[i])\n",
        "            axes[i].set_title(f'{col} by Attack Type')\n",
        "            axes[i].set_xlabel('Attack Type')\n",
        "            axes[i].set_ylabel(col)\n",
        "            axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(figures_dir, 'packet_counts_by_attack.png'))\n",
        "        plt.close()\n",
        "\n",
        "    # PCA visualization for attack types\n",
        "    # Select numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    # Exclude the encoded labels\n",
        "    if 'attack_encoded' in numeric_cols:\n",
        "        numeric_cols.remove('attack_encoded')\n",
        "\n",
        "    if len(numeric_cols) > 2:  # Need at least 2 features for PCA\n",
        "        try:\n",
        "            # Standardize the features\n",
        "            X = df[numeric_cols]\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "            # Apply PCA\n",
        "            pca = PCA(n_components=2)\n",
        "            X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "            # Create a dataframe for plotting\n",
        "            pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "            pca_df['attack_label'] = df['attack_label'].values\n",
        "\n",
        "            # Plot PCA results\n",
        "            plt.figure(figsize=(14, 10))\n",
        "\n",
        "            attack_types = pca_df['attack_label'].unique()\n",
        "\n",
        "            # Create colormap with distinct colors\n",
        "            cmap = cm.get_cmap('tab20', len(attack_types))\n",
        "\n",
        "            for i, attack in enumerate(attack_types):\n",
        "                idx = pca_df['attack_label'] == attack\n",
        "                plt.scatter(pca_df.loc[idx, 'PC1'], pca_df.loc[idx, 'PC2'],\n",
        "                          label=attack, alpha=0.7, s=50, color=cmap(i))\n",
        "\n",
        "            plt.title('PCA of Attack Types')\n",
        "            plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "            plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(figures_dir, 'pca_attack_types.png'))\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"PCA completed. Total explained variance: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in PCA visualization: {e}\")\n",
        "\n",
        "def correlation_analysis(df, figures_dir):\n",
        "    \"\"\"Perform correlation analysis between features and attack types.\"\"\"\n",
        "    print(\"\\n--- Performing Correlation Analysis ---\")\n",
        "\n",
        "    # Get numeric columns (excluding encoded labels)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'attack_encoded' in numeric_cols:\n",
        "        numeric_cols.remove('attack_encoded')\n",
        "\n",
        "    # Correlation between features\n",
        "    if len(numeric_cols) > 1:\n",
        "        try:\n",
        "            plt.figure(figsize=(16, 14))\n",
        "            corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "            # Create heatmap\n",
        "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "            sns.heatmap(corr_matrix, mask=mask, cmap=\"coolwarm\", center=0,\n",
        "                        square=True, linewidths=.5, annot=False, fmt='.2f',\n",
        "                        vmin=-1, vmax=1)\n",
        "            plt.title('Feature Correlation Matrix')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(figures_dir, 'feature_correlation_matrix.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # For better readability, create a filtered correlation matrix showing only high correlations\n",
        "            high_corr = corr_matrix.abs().unstack().sort_values(kind=\"quicksort\", ascending=False)\n",
        "            high_corr = high_corr[high_corr < 1.0]  # Remove self-correlations (1.0)\n",
        "            high_corr = high_corr[high_corr > 0.5]  # Keep only high correlations\n",
        "\n",
        "            if not high_corr.empty:\n",
        "                high_corr_df = pd.DataFrame(high_corr.reset_index())\n",
        "                high_corr_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "\n",
        "                plt.figure(figsize=(12, len(high_corr_df) * 0.4))\n",
        "                sns.barplot(x='Correlation', y='Feature 1', hue='Feature 2', data=high_corr_df)\n",
        "                plt.title('High Feature Correlations (|r| > 0.5)')\n",
        "                plt.xlabel('Correlation Coefficient')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(figures_dir, 'high_feature_correlations.png'))\n",
        "                plt.close()\n",
        "\n",
        "                print(f\"Found {len(high_corr_df)} feature pairs with high correlation (|r| > 0.5)\")\n",
        "            else:\n",
        "                print(\"No high correlations (|r| > 0.5) found between features\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in correlation analysis: {e}\")\n",
        "\n",
        "    # Correlation of features with attack types (using point-biserial correlation for each attack type)\n",
        "    attack_types = df['attack_label'].unique()\n",
        "\n",
        "    # Create a figure for feature importance by attack type\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # For each attack type, calculate correlation with features\n",
        "    correlations = {}\n",
        "\n",
        "    try:\n",
        "        for attack in attack_types:\n",
        "            # Create binary column (1 for this attack, 0 for others)\n",
        "            df[f'is_{attack}'] = (df['attack_label'] == attack).astype(int)\n",
        "\n",
        "            # Calculate correlation of each feature with this attack type\n",
        "            attack_corr = df[numeric_cols].corrwith(df[f'is_{attack}'])\n",
        "            correlations[attack] = attack_corr\n",
        "\n",
        "            # Remove the temporary column\n",
        "            df.drop(f'is_{attack}', axis=1, inplace=True)\n",
        "\n",
        "        # Convert to dataframe for easier manipulation\n",
        "        corr_df = pd.DataFrame(correlations)\n",
        "\n",
        "        # Sort features by their maximum absolute correlation with any attack type\n",
        "        corr_df['max_abs_corr'] = corr_df.abs().max(axis=1)\n",
        "        corr_df = corr_df.sort_values('max_abs_corr', ascending=False)\n",
        "\n",
        "        # Take top 15 features for visualization\n",
        "        top_features = corr_df.head(15).index.tolist()\n",
        "\n",
        "        # Create heatmap of feature-attack correlations\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        sns.heatmap(corr_df.loc[top_features, attack_types], cmap=\"coolwarm\", center=0,\n",
        "                   annot=True, fmt='.2f', linewidths=.5, vmin=-1, vmax=1)\n",
        "        plt.title('Correlation between Top Features and Attack Types')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(figures_dir, 'feature_attack_correlation.png'))\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Feature-attack correlation analysis completed for {len(attack_types)} attack types\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature-attack correlation analysis: {e}\")\n",
        "\n",
        "    # Return for use in the feature importance analysis\n",
        "    return numeric_cols\n",
        "\n",
        "def feature_importance_analysis_optimized(df, numeric_cols, figures_dir):\n",
        "    \"\"\"Analyze feature importance for attack classification using Random Forest with memory optimization.\"\"\"\n",
        "    print(\"\\n--- Analyzing Feature Importance (Optimized) ---\")\n",
        "\n",
        "    # Take a sample if the dataset is very large\n",
        "    sample_size = 100000  # Adjust this based on your available memory\n",
        "    if len(df) > sample_size:\n",
        "        print(f\"Dataset is large ({len(df)} rows). Using a sample of {sample_size} rows for feature importance analysis.\")\n",
        "        df_sample = df.sample(sample_size, random_state=42)\n",
        "    else:\n",
        "        df_sample = df\n",
        "\n",
        "    try:\n",
        "        # Prepare data for model with fewer estimators and limited depth\n",
        "        X = df_sample[numeric_cols]\n",
        "        y = df_sample['attack_encoded']\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Train a lighter Random Forest Classifier\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=50,       # Reduced from 100\n",
        "            max_depth=10,          # Limit tree depth\n",
        "            min_samples_split=5,   # Require more samples to split\n",
        "            n_jobs=4,              # Limit parallelism\n",
        "            random_state=42\n",
        "        )\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        # Get feature importances\n",
        "        feature_importances = pd.DataFrame({\n",
        "            'feature': numeric_cols,\n",
        "            'importance': rf.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Visualize top 20 features\n",
        "        top_n = 20\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        if len(feature_importances) > top_n:\n",
        "            top_features = feature_importances.head(top_n)\n",
        "        else:\n",
        "            top_features = feature_importances\n",
        "\n",
        "        sns.barplot(x='importance', y='feature', data=top_features)\n",
        "        plt.title(f'Top {len(top_features)} Features by Importance')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(figures_dir, 'feature_importance.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Use a smaller subset for evaluation\n",
        "        if len(X_test) > 10000:\n",
        "            print(f\"Using a subset of 10,000 samples for evaluation\")\n",
        "            X_test_small = X_test.iloc[:10000]\n",
        "            y_test_small = y_test.iloc[:10000]\n",
        "        else:\n",
        "            X_test_small = X_test\n",
        "            y_test_small = y_test\n",
        "\n",
        "        # Evaluate on a smaller test set\n",
        "        y_pred = rf.predict(X_test_small)\n",
        "        accuracy = accuracy_score(y_test_small, y_pred)\n",
        "\n",
        "        print(f\"Random Forest model accuracy on sample: {accuracy:.4f}\")\n",
        "\n",
        "        # Create simplified confusion matrix with top attack types\n",
        "        attack_counts = df_sample['attack_encoded'].value_counts()\n",
        "        top_attacks = attack_counts.nlargest(10).index.tolist()\n",
        "\n",
        "        # Filter test data to only include top attacks\n",
        "        top_attacks_mask = np.isin(y_test_small, top_attacks)\n",
        "        y_test_top = y_test_small[top_attacks_mask]\n",
        "        y_pred_top = y_pred[top_attacks_mask]\n",
        "\n",
        "        if len(y_test_top) > 0:\n",
        "            cm = confusion_matrix(y_test_top, y_pred_top)\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Get class names for the top attacks\n",
        "            attack_labels = [df_sample.loc[df_sample['attack_encoded'] == attack, 'attack_label'].iloc[0]\n",
        "                             for attack in top_attacks if attack in y_test_top.unique()]\n",
        "\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=attack_labels,\n",
        "                       yticklabels=attack_labels)\n",
        "            plt.title('Confusion Matrix (Top Attack Types)')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.yticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(figures_dir, 'confusion_matrix_top_attacks.png'))\n",
        "            plt.close()\n",
        "\n",
        "        return feature_importances.head(10)['feature'].tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature importance analysis: {e}\")\n",
        "        return []\n",
        "\n",
        "def create_summary_dashboard(df, top_features, figures_dir):\n",
        "    \"\"\"Create a summary dashboard with key insights.\"\"\"\n",
        "    print(\"\\n--- Creating Summary Dashboard ---\")\n",
        "\n",
        "    try:\n",
        "        # Create a dashboard with multiple plots\n",
        "        fig = plt.figure(figsize=(20, 24))\n",
        "        gs = gridspec.GridSpec(4, 2, figure=fig)\n",
        "\n",
        "        # 1. Attack Distribution Pie Chart\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        attack_counts = df['attack_label'].value_counts()\n",
        "        attack_counts.plot.pie(autopct='%1.1f%%', textprops={'fontsize': 9},\n",
        "                              colors=plt.cm.tab20.colors, ax=ax1)\n",
        "        ax1.set_title('Attack Type Distribution')\n",
        "        ax1.set_ylabel('')\n",
        "\n",
        "        # 2. Top Features Bar Chart\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        if top_features and all(feat in df.columns for feat in top_features):\n",
        "            # Get mean values of top features by attack type\n",
        "            feature_means = df.groupby('attack_label')[top_features[:5]].mean()\n",
        "            feature_means.plot(kind='bar', ax=ax2)\n",
        "            ax2.set_title('Top 5 Features by Attack Type')\n",
        "            ax2.tick_params(axis='x', rotation=45)\n",
        "            ax2.set_xlabel('')\n",
        "            ax2.legend(loc='upper right')\n",
        "\n",
        "        # 3. Protocol Distribution if available\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        if 'Protocol' in df.columns:\n",
        "            protocol_counts = df['Protocol'].value_counts()\n",
        "            protocol_counts.plot(kind='bar', ax=ax3, color='skyblue')\n",
        "            ax3.set_title('Protocol Distribution')\n",
        "            ax3.set_xlabel('Protocol')\n",
        "            ax3.set_ylabel('Count')\n",
        "        else:\n",
        "            ax3.set_title('Protocol Distribution (Data Not Available)')\n",
        "            ax3.axis('off')\n",
        "\n",
        "        # 4. Flow Duration Histogram if available\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        if 'Flow Duration' in df.columns:\n",
        "            df['Flow Duration'].hist(bins=50, ax=ax4, color='lightgreen')\n",
        "            ax4.set_title('Flow Duration Distribution')\n",
        "            ax4.set_xlabel('Flow Duration')\n",
        "            ax4.set_ylabel('Frequency')\n",
        "            # Use log scale if values span multiple orders of magnitude\n",
        "            if df['Flow Duration'].max() / (df['Flow Duration'].min() + 1) > 1000:\n",
        "                ax4.set_xscale('log')\n",
        "        else:\n",
        "            ax4.set_title('Flow Duration Distribution (Data Not Available)')\n",
        "            ax4.axis('off')\n",
        "\n",
        "        # 5. Flow Bytes/s by Attack Type if available\n",
        "        ax5 = fig.add_subplot(gs[2, :])\n",
        "        flowbytes_cols = [col for col in df.columns if 'bytes' in col.lower()]\n",
        "\n",
        "        if flowbytes_cols:\n",
        "            col = flowbytes_cols[0]\n",
        "            boxplot = sns.boxplot(x='attack_label', y=col, data=df, ax=ax5)\n",
        "            ax5.set_title(f'{col} by Attack Type')\n",
        "            ax5.set_xlabel('Attack Type')\n",
        "            ax5.set_ylabel(col)\n",
        "            ax5.tick_params(axis='x', rotation=45)\n",
        "\n",
        "            # If values span multiple orders, use log scale\n",
        "            if df[col].max() / (df[col].min() + 1) > 1000:\n",
        "                ax5.set_yscale('log')\n",
        "        else:\n",
        "            ax5.set_title('Flow Bytes Analysis (No appropriate columns available)')\n",
        "            ax5.axis('off')\n",
        "\n",
        "        # 6. Key Statistics Table\n",
        "        ax6 = fig.add_subplot(gs[3, :])\n",
        "        ax6.axis('tight')\n",
        "        ax6.axis('off')\n",
        "\n",
        "        # Gather key statistics\n",
        "        total_samples = len(df)\n",
        "        attack_pct = (df['attack_label'] != 'BENIGN').mean() * 100 if 'BENIGN' in df['attack_label'].unique() else 100\n",
        "        most_common_attack = df['attack_label'].value_counts().index[0]\n",
        "        attack_count = df['attack_label'].value_counts().iloc[0]\n",
        "        attack_percent = attack_count / total_samples * 100\n",
        "\n",
        "        # Create data for the table\n",
        "        table_data = [\n",
        "            ['Total Samples', f\"{total_samples:,}\"],\n",
        "            ['Attack Percentage', f\"{attack_pct:.2f}%\"],\n",
        "            ['Most Common Attack', most_common_attack],\n",
        "            ['Count of Most Common Attack', f\"{attack_count:,} ({attack_percent:.2f}%)\"],\n",
        "            ['Number of Attack Types', f\"{df['attack_label'].nunique()}\"],\n",
        "        ]\n",
        "\n",
        "        # Add more statistics if columns are available\n",
        "        if 'Protocol' in df.columns:\n",
        "            table_data.append(['Most Common Protocol', df['Protocol'].value_counts().index[0]])\n",
        "\n",
        "        flowbytes_cols = [col for col in df.columns if 'bytes' in col.lower()]\n",
        "        if flowbytes_cols:\n",
        "            max_flow_attack = df.groupby('attack_label')[flowbytes_cols[0]].mean().idxmax()\n",
        "            table_data.append(['Attack with Max Flow Bytes (avg)', max_flow_attack])\n",
        "\n",
        "        # Create the table\n",
        "        table = ax6.table(cellText=table_data, colLabels=['Statistic', 'Value'],\n",
        "                          loc='center', cellLoc='left')\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(12)\n",
        "        table.scale(1, 2)\n",
        "\n",
        "        plt.suptitle('IoT Cyber Attack Analysis Dashboard', fontsize=20)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
        "        plt.savefig(os.path.join(figures_dir, 'summary_dashboard.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"Summary dashboard created\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating summary dashboard: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute the entire analysis pipeline.\"\"\"\n",
        "    print(\"Starting IoT Cyber Attack Analysis\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    base_path = mount_drive()\n",
        "\n",
        "    # Create output directory\n",
        "    figures_dir = create_output_dir(base_path)\n",
        "\n",
        "    try:\n",
        "        # Load data from all CSV files\n",
        "        data = load_data(base_path)\n",
        "\n",
        "        # Preprocess the data\n",
        "        processed_data, label_mapping = preprocess_data(data)\n",
        "\n",
        "        # Perform exploratory data analysis\n",
        "        exploratory_data_analysis(processed_data, label_mapping, figures_dir)\n",
        "\n",
        "        # Perform correlation analysis\n",
        "        numeric_cols = correlation_analysis(processed_data, figures_dir)\n",
        "\n",
        "        # Analyze feature importance (using optimized function to avoid memory issues)\n",
        "        top_features = feature_importance_analysis_optimized(processed_data, numeric_cols, figures_dir)\n",
        "\n",
        "        # Create summary dashboard\n",
        "        create_summary_dashboard(processed_data, top_features, figures_dir)\n",
        "\n",
        "        print(\"\\n--- Analysis Complete ---\")\n",
        "        print(f\"All visualizations have been saved to the '{figures_dir}' directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}